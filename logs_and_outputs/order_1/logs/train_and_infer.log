/home/yangye/O-LoRA
Host=server-a100-8g-1  SubmitDir=/home/yangye
JobID=782  ArrayIndex=0
        GPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID
GPU0     X      NV12    20-23   0               N/A
GPU1    NV12     X              1               N/A

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks
[2026-02-25 23:46:39,188] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-25 23:46:40,117] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2026-02-25 23:46:40,187] [INFO] [runner.py:555:main] cmd = /workspace/envs/MLLMs/yangye/conda_envs/lora/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=28536 --enable_each_rank_log=None src/run_uie_lora.py --do_train --do_predict --predict_with_generate --model_name_or_path /workspace/data/MLLMs/yangye/initial_model/t5-large --data_dir CL_Benchmark --task_config_dir configs/order1_configs/dbpedia --instruction_file configs/instruction_config.json --instruction_strategy single --output_dir logs_and_outputs/order_1/outputs/1-dbpedia --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --num_train_epochs 1 --deepspeed configs/ds_configs/stage2.config --run_name order1_round1 --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --evaluation_strategy no --save_strategy no --save_steps 1500 --lamda_1 0.5 --lamda_2 0
[2026-02-25 23:46:41,419] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-25 23:46:42,332] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2026-02-25 23:46:42,333] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2026-02-25 23:46:42,333] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2026-02-25 23:46:42,333] [INFO] [launch.py:163:main] dist_world_size=2
[2026-02-25 23:46:42,333] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2026-02-25 23:46:44,431] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-25 23:46:44,451] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-25 23:46:45,293] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2026-02-25 23:46:45,293] [INFO] [comm.py:616:init_distributed] cdb=None
[2026-02-25 23:46:45,293] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2026-02-25 23:46:45,305] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2026-02-25 23:46:45,305] [INFO] [comm.py:616:init_distributed] cdb=None
02/25/2026 23:46:46 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
02/25/2026 23:46:46 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
02/25/2026 23:46:46 - WARNING - datasets.builder - Using custom data configuration default-8a6abe4434e90f8a
02/25/2026 23:46:46 - WARNING - datasets.builder - Reusing dataset uie_instructions (logs_and_outputs/order_1/outputs/1-dbpedia/17925c6a81583bc3da5a4b7d196aba0a/uie_instructions/default-8a6abe4434e90f8a/2.0.0/c490e7f13dec80785fc335819009163a45c86ae2816040c8d81800108e7e4374)
02/25/2026 23:46:46 - WARNING - datasets.builder - Using custom data configuration default-8a6abe4434e90f8a
02/25/2026 23:46:46 - WARNING - datasets.builder - Reusing dataset uie_instructions (logs_and_outputs/order_1/outputs/1-dbpedia/17925c6a81583bc3da5a4b7d196aba0a/uie_instructions/default-8a6abe4434e90f8a/2.0.0/c490e7f13dec80785fc335819009163a45c86ae2816040c8d81800108e7e4374)
-----Gradient checkpointing: False -----
-----Gradient checkpointing: False -----
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
ninja: no work to do.
Time to load cpu_adam op: 2.94105863571167 seconds
Rank: 1 partition count [2] and sizes[(1179648, False)] 
Time to load cpu_adam op: 2.993917942047119 seconds
Rank: 0 partition count [2] and sizes[(1179648, False)] 
{'loss': 2.5412, 'learning_rate': 0.001, 'epoch': 0.05}
{'loss': 0.1776, 'learning_rate': 0.001, 'epoch': 0.09}
{'loss': 0.0742, 'learning_rate': 0.001, 'epoch': 0.14}
{'loss': 0.0752, 'learning_rate': 0.001, 'epoch': 0.18}
{'loss': 0.0316, 'learning_rate': 0.001, 'epoch': 0.23}
{'loss': 0.0506, 'learning_rate': 0.001, 'epoch': 0.27}
{'loss': 0.0399, 'learning_rate': 0.001, 'epoch': 0.32}
{'loss': 0.0323, 'learning_rate': 0.001, 'epoch': 0.37}
{'loss': 0.041, 'learning_rate': 0.001, 'epoch': 0.41}
{'loss': 0.0299, 'learning_rate': 0.001, 'epoch': 0.46}
{'loss': 0.0288, 'learning_rate': 0.001, 'epoch': 0.5}
{'loss': 0.0324, 'learning_rate': 0.001, 'epoch': 0.55}
{'loss': 0.0226, 'learning_rate': 0.001, 'epoch': 0.59}
{'loss': 0.0358, 'learning_rate': 0.001, 'epoch': 0.64}
{'loss': 0.0272, 'learning_rate': 0.001, 'epoch': 0.69}
{'loss': 0.01, 'learning_rate': 0.001, 'epoch': 0.73}
{'loss': 0.0172, 'learning_rate': 0.001, 'epoch': 0.78}
{'loss': 0.0236, 'learning_rate': 0.001, 'epoch': 0.82}
{'loss': 0.0185, 'learning_rate': 0.001, 'epoch': 0.87}
{'loss': 0.0381, 'learning_rate': 0.001, 'epoch': 0.91}
{'loss': 0.0331, 'learning_rate': 0.001, 'epoch': 0.96}
{'train_runtime': 506.1504, 'train_samples_per_second': 27.66, 'train_steps_per_second': 0.431, 'train_loss': 0.1563156862324531, 'epoch': 1.0}
***** train metrics *****
  epoch                    =        1.0
  train_loss               =     0.1563
  train_runtime            = 0:08:26.15
  train_samples            =      14000
  train_samples_per_second =      27.66
  train_steps_per_second   =      0.431
***** predict metrics *****
  epoch                           =        1.0
  predict_exact_match             =    98.5658
  predict_exact_match_for_TC      =    98.5658
  predict_exact_match_for_dbpedia =    98.5658
  predict_gen_len                 =     2.6507
  predict_global_step             =        218
  predict_loss                    =     0.0252
  predict_rouge1                  =    98.5658
  predict_rouge1_for_TC           =    98.5658
  predict_rouge1_for_dbpedia      =    98.5658
  predict_rougeL                  =    98.5658
  predict_rougeL_for_TC           =    98.5658
  predict_rougeL_for_dbpedia      =    98.5658
  predict_runtime                 = 0:00:34.15
  predict_samples                 =       7600
  predict_samples_per_second      =     222.49
  predict_steps_per_second        =      0.878
[2026-02-25 23:56:09,407] [INFO] [launch.py:347:main] Process 1571246 exits successfully.
[2026-02-25 23:56:09,408] [INFO] [launch.py:347:main] Process 1571245 exits successfully.
[2026-02-25 23:56:17,919] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-25 23:56:18,848] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2026-02-25 23:56:18,916] [INFO] [runner.py:555:main] cmd = /workspace/envs/MLLMs/yangye/conda_envs/lora/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=28536 --enable_each_rank_log=None src/run_uie_lora.py --do_train --do_predict --predict_with_generate --model_name_or_path logs_and_outputs/order_1/outputs/1-dbpedia/adapter --data_dir CL_Benchmark --task_config_dir configs/order1_configs/amazon --instruction_file configs/instruction_config.json --instruction_strategy single --output_dir logs_and_outputs/order_1/outputs/2-amazon --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --num_train_epochs 1 --deepspeed configs/ds_configs/stage2.config --run_name order1_round2 --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --evaluation_strategy no --save_strategy no --save_steps 1500 --lamda_1 0.5 --lamda_2 0
[2026-02-25 23:56:20,147] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-25 23:56:21,061] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2026-02-25 23:56:21,061] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2026-02-25 23:56:21,061] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2026-02-25 23:56:21,061] [INFO] [launch.py:163:main] dist_world_size=2
[2026-02-25 23:56:21,061] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2026-02-25 23:56:23,173] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-25 23:56:23,191] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-25 23:56:24,029] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2026-02-25 23:56:24,030] [INFO] [comm.py:616:init_distributed] cdb=None
[2026-02-25 23:56:24,030] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2026-02-25 23:56:24,045] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2026-02-25 23:56:24,045] [INFO] [comm.py:616:init_distributed] cdb=None
02/25/2026 23:56:24 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
02/25/2026 23:56:24 - WARNING - datasets.builder - Using custom data configuration default-25178e251dd03be0
02/25/2026 23:56:24 - WARNING - datasets.builder - Reusing dataset uie_instructions (logs_and_outputs/order_1/outputs/2-amazon/f236d5f0073f6b88182545d96de70db6/uie_instructions/default-25178e251dd03be0/2.0.0/c490e7f13dec80785fc335819009163a45c86ae2816040c8d81800108e7e4374)
02/25/2026 23:56:24 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
02/25/2026 23:56:24 - WARNING - datasets.builder - Using custom data configuration default-25178e251dd03be0
02/25/2026 23:56:24 - WARNING - datasets.builder - Reusing dataset uie_instructions (logs_and_outputs/order_1/outputs/2-amazon/f236d5f0073f6b88182545d96de70db6/uie_instructions/default-25178e251dd03be0/2.0.0/c490e7f13dec80785fc335819009163a45c86ae2816040c8d81800108e7e4374)
-----Gradient checkpointing: False -----
-----Gradient checkpointing: False -----
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
ninja: no work to do.
Time to load cpu_adam op: 2.9477267265319824 seconds
Rank: 1 partition count [2] and sizes[(1179648, False)] 
Time to load cpu_adam op: 3.010357618331909 seconds
Rank: 0 partition count [2] and sizes[(1179648, False)] 
{'loss': 28.5328, 'learning_rate': 0.001, 'epoch': 0.13}
{'loss': 14.4125, 'learning_rate': 0.001, 'epoch': 0.26}
{'loss': 9.5336, 'learning_rate': 0.001, 'epoch': 0.38}
{'loss': 7.0805, 'learning_rate': 0.001, 'epoch': 0.51}
{'loss': 5.9395, 'learning_rate': 0.001, 'epoch': 0.64}
{'loss': 5.6293, 'learning_rate': 0.001, 'epoch': 0.77}
{'loss': 5.5066, 'learning_rate': 0.001, 'epoch': 0.89}
{'train_runtime': 187.324, 'train_samples_per_second': 26.692, 'train_steps_per_second': 0.416, 'train_loss': 10.380258413461538, 'epoch': 1.0}
***** train metrics *****
  epoch                    =        1.0
  train_loss               =    10.3803
  train_runtime            = 0:03:07.32
  train_samples            =       5000
  train_samples_per_second =     26.692
  train_steps_per_second   =      0.416
***** predict metrics *****
  epoch                           =        1.0
  predict_exact_match             =    76.8618
  predict_exact_match_for_SC      =    55.4737
  predict_exact_match_for_TC      =      98.25
  predict_exact_match_for_amazon  =    55.4737
  predict_exact_match_for_dbpedia =      98.25
  predict_gen_len                 =     2.5616
  predict_global_step             =         78
  predict_loss                    =     0.2258
  predict_rouge1                  =    83.1239
  predict_rouge1_for_SC           =    67.9978
  predict_rouge1_for_TC           =      98.25
  predict_rouge1_for_amazon       =    67.9978
  predict_rouge1_for_dbpedia      =      98.25
  predict_rougeL                  =    83.1239
  predict_rougeL_for_SC           =    67.9978
  predict_rougeL_for_TC           =      98.25
  predict_rougeL_for_amazon       =    67.9978
  predict_rougeL_for_dbpedia      =      98.25
  predict_runtime                 = 0:01:16.24
  predict_samples                 =      15200
  predict_samples_per_second      =     199.37
  predict_steps_per_second        =      0.787
[2026-02-26 00:01:11,098] [INFO] [launch.py:347:main] Process 1573089 exits successfully.
[2026-02-26 00:01:12,099] [INFO] [launch.py:347:main] Process 1573090 exits successfully.
[2026-02-26 00:01:20,603] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-26 00:01:21,531] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2026-02-26 00:01:21,605] [INFO] [runner.py:555:main] cmd = /workspace/envs/MLLMs/yangye/conda_envs/lora/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=28536 --enable_each_rank_log=None src/run_uie_lora.py --do_train --do_predict --predict_with_generate --model_name_or_path logs_and_outputs/order_1/outputs/2-amazon/adapter --data_dir CL_Benchmark --task_config_dir configs/order1_configs/yahoo --instruction_file configs/instruction_config.json --instruction_strategy single --output_dir logs_and_outputs/order_1/outputs/3-yahoo --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --num_train_epochs 1 --deepspeed configs/ds_configs/stage2.config --run_name order1_round3 --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --evaluation_strategy no --save_strategy no --save_steps 1500 --lamda_1 0.5 --lamda_2 0
[2026-02-26 00:01:22,837] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-26 00:01:23,752] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2026-02-26 00:01:23,753] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2026-02-26 00:01:23,753] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2026-02-26 00:01:23,753] [INFO] [launch.py:163:main] dist_world_size=2
[2026-02-26 00:01:23,753] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2026-02-26 00:01:25,851] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-26 00:01:25,873] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-26 00:01:26,713] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2026-02-26 00:01:26,713] [INFO] [comm.py:616:init_distributed] cdb=None
[2026-02-26 00:01:26,730] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2026-02-26 00:01:26,730] [INFO] [comm.py:616:init_distributed] cdb=None
[2026-02-26 00:01:26,730] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
02/26/2026 00:01:28 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
02/26/2026 00:01:28 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
02/26/2026 00:01:28 - WARNING - datasets.builder - Using custom data configuration default-d971c7bc1435f4ac
02/26/2026 00:01:28 - WARNING - datasets.builder - Reusing dataset uie_instructions (logs_and_outputs/order_1/outputs/3-yahoo/b3f2d9fb75813b68e82efb3b6bab35bd/uie_instructions/default-d971c7bc1435f4ac/2.0.0/c490e7f13dec80785fc335819009163a45c86ae2816040c8d81800108e7e4374)
02/26/2026 00:01:28 - WARNING - datasets.builder - Using custom data configuration default-d971c7bc1435f4ac
02/26/2026 00:01:28 - WARNING - datasets.builder - Reusing dataset uie_instructions (logs_and_outputs/order_1/outputs/3-yahoo/b3f2d9fb75813b68e82efb3b6bab35bd/uie_instructions/default-d971c7bc1435f4ac/2.0.0/c490e7f13dec80785fc335819009163a45c86ae2816040c8d81800108e7e4374)
-----Gradient checkpointing: False -----
-----Gradient checkpointing: False -----
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
ninja: no work to do.
Time to load cpu_adam op: 2.9820716381073 seconds
Rank: 0 partition count [2] and sizes[(1179648, False)] 
Time to load cpu_adam op: 3.056490182876587 seconds
Rank: 1 partition count [2] and sizes[(1179648, False)] 
{'loss': 48.25, 'learning_rate': 0.001, 'epoch': 0.06}
{'loss': 22.2203, 'learning_rate': 0.001, 'epoch': 0.13}
{'loss': 12.9883, 'learning_rate': 0.001, 'epoch': 0.19}
{'loss': 9.3891, 'learning_rate': 0.001, 'epoch': 0.26}
{'loss': 8.1047, 'learning_rate': 0.001, 'epoch': 0.32}
{'loss': 7.1492, 'learning_rate': 0.001, 'epoch': 0.38}
{'loss': 6.7766, 'learning_rate': 0.001, 'epoch': 0.45}
{'loss': 6.5633, 'learning_rate': 0.001, 'epoch': 0.51}
{'loss': 6.4797, 'learning_rate': 0.001, 'epoch': 0.58}
{'loss': 6.2637, 'learning_rate': 0.001, 'epoch': 0.64}
{'loss': 6.3242, 'learning_rate': 0.001, 'epoch': 0.7}
{'loss': 6.2977, 'learning_rate': 0.001, 'epoch': 0.77}
{'loss': 6.2078, 'learning_rate': 0.001, 'epoch': 0.83}
{'loss': 6.2422, 'learning_rate': 0.001, 'epoch': 0.9}
{'loss': 6.2043, 'learning_rate': 0.001, 'epoch': 0.96}
{'train_runtime': 384.5212, 'train_samples_per_second': 26.006, 'train_steps_per_second': 0.406, 'train_loss': 10.846153846153847, 'epoch': 1.0}
***** train metrics *****
  epoch                    =        1.0
  train_loss               =    10.8462
  train_runtime            = 0:06:24.52
  train_samples            =      10000
  train_samples_per_second =     26.006
  train_steps_per_second   =      0.406
***** predict metrics *****
  epoch                           =        1.0
  predict_exact_match             =    74.7412
  predict_exact_match_for_SC      =    54.8158
  predict_exact_match_for_TC      =    84.7039
  predict_exact_match_for_amazon  =    54.8158
  predict_exact_match_for_dbpedia =    97.6974
  predict_exact_match_for_yahoo   =    71.7105
  predict_gen_len                 =     3.2559
  predict_global_step             =        156
  predict_loss                    =     0.2226
  predict_rouge1                  =    79.3494
  predict_rouge1_for_SC           =    68.6404
  predict_rouge1_for_TC           =    84.7039
  predict_rouge1_for_amazon       =    68.6404
  predict_rouge1_for_dbpedia      =    97.6974
  predict_rouge1_for_yahoo        =    71.7105
  predict_rougeL                  =    79.3494
  predict_rougeL_for_SC           =    68.6404
  predict_rougeL_for_TC           =    84.7039
  predict_rougeL_for_amazon       =    68.6404
  predict_rougeL_for_dbpedia      =    97.6974
  predict_rougeL_for_yahoo        =    71.7105
  predict_runtime                 = 0:02:28.28
  predict_samples                 =      22800
  predict_samples_per_second      =    153.754
  predict_steps_per_second        =      0.607
[2026-02-26 00:10:43,835] [INFO] [launch.py:347:main] Process 1582108 exits successfully.
[2026-02-26 00:10:44,836] [INFO] [launch.py:347:main] Process 1582107 exits successfully.
[2026-02-26 00:10:53,347] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-26 00:10:54,277] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2026-02-26 00:10:54,353] [INFO] [runner.py:555:main] cmd = /workspace/envs/MLLMs/yangye/conda_envs/lora/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=28536 --enable_each_rank_log=None src/run_uie_lora.py --do_train --do_predict --predict_with_generate --model_name_or_path logs_and_outputs/order_1/outputs/3-yahoo/adapter --data_dir CL_Benchmark --task_config_dir configs/order1_configs/agnews --instruction_file configs/instruction_config.json --instruction_strategy single --output_dir logs_and_outputs/order_1/outputs/4-agnews --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --num_train_epochs 1 --deepspeed configs/ds_configs/stage2.config --run_name order1_round4 --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --evaluation_strategy no --save_strategy no --save_steps 1500 --lamda_1 0.5 --lamda_2 0
[2026-02-26 00:10:55,587] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-26 00:10:56,503] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2026-02-26 00:10:56,503] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2026-02-26 00:10:56,503] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2026-02-26 00:10:56,503] [INFO] [launch.py:163:main] dist_world_size=2
[2026-02-26 00:10:56,504] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2026-02-26 00:10:58,592] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-26 00:10:58,608] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-02-26 00:10:59,450] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2026-02-26 00:10:59,450] [INFO] [comm.py:616:init_distributed] cdb=None
[2026-02-26 00:10:59,450] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2026-02-26 00:10:59,465] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2026-02-26 00:10:59,465] [INFO] [comm.py:616:init_distributed] cdb=None
02/26/2026 00:11:00 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
02/26/2026 00:11:00 - WARNING - datasets.builder - Using custom data configuration default-c1ffe425bee4d951
02/26/2026 00:11:00 - WARNING - datasets.builder - Reusing dataset uie_instructions (logs_and_outputs/order_1/outputs/4-agnews/895e4cf9c1b5eba6adaea9579a065a61/uie_instructions/default-c1ffe425bee4d951/2.0.0/c490e7f13dec80785fc335819009163a45c86ae2816040c8d81800108e7e4374)
02/26/2026 00:11:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
02/26/2026 00:11:00 - WARNING - datasets.builder - Using custom data configuration default-c1ffe425bee4d951
02/26/2026 00:11:00 - WARNING - datasets.builder - Reusing dataset uie_instructions (logs_and_outputs/order_1/outputs/4-agnews/895e4cf9c1b5eba6adaea9579a065a61/uie_instructions/default-c1ffe425bee4d951/2.0.0/c490e7f13dec80785fc335819009163a45c86ae2816040c8d81800108e7e4374)
-----Gradient checkpointing: False -----
-----Gradient checkpointing: False -----
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
ninja: no work to do.
Time to load cpu_adam op: 2.9446191787719727 seconds
Rank: 1 partition count [2] and sizes[(1179648, False)] 
Time to load cpu_adam op: 2.9973185062408447 seconds
Rank: 0 partition count [2] and sizes[(1179648, False)] 
{'loss': 65.4813, 'learning_rate': 0.001, 'epoch': 0.16}
{'loss': 29.4766, 'learning_rate': 0.001, 'epoch': 0.32}
{'loss': 16.9914, 'learning_rate': 0.001, 'epoch': 0.48}
{'loss': 11.0398, 'learning_rate': 0.001, 'epoch': 0.64}
{'loss': 9.2547, 'learning_rate': 0.001, 'epoch': 0.8}
{'loss': 8.7148, 'learning_rate': 0.001, 'epoch': 0.96}
{'train_runtime': 147.1833, 'train_samples_per_second': 27.177, 'train_steps_per_second': 0.421, 'train_loss': 23.009198588709676, 'epoch': 0.99}
***** train metrics *****
  epoch                    =       0.99
  train_loss               =    23.0092
  train_runtime            = 0:02:27.18
  train_samples            =       4000
  train_samples_per_second =     27.177
  train_steps_per_second   =      0.421
***** predict metrics *****
  epoch                           =       0.99
  predict_exact_match             =    77.9046
  predict_exact_match_for_SC      =    55.3947
  predict_exact_match_for_TC      =    85.4079
  predict_exact_match_for_agnews  =    88.4605
  predict_exact_match_for_amazon  =    55.3947
  predict_exact_match_for_dbpedia =    97.8026
  predict_exact_match_for_yahoo   =    69.9605
  predict_gen_len                 =     3.1071
  predict_global_step             =         62
  predict_loss                    =      0.206
  predict_rouge1                  =    81.0033
  predict_rouge1_for_SC           =    67.7895
  predict_rouge1_for_TC           =    85.4079
  predict_rouge1_for_agnews       =    88.4605
  predict_rouge1_for_amazon       =    67.7895
  predict_rouge1_for_dbpedia      =    97.8026
  predict_rouge1_for_yahoo        =    69.9605
  predict_rougeL                  =    81.0033
  predict_rougeL_for_SC           =    67.7895
  predict_rougeL_for_TC           =    85.4079
  predict_rougeL_for_agnews       =    88.4605
  predict_rougeL_for_amazon       =    67.7895
  predict_rougeL_for_dbpedia      =    97.8026
  predict_rougeL_for_yahoo        =    69.9605
  predict_runtime                 = 0:02:57.45
  predict_samples                 =      30400
  predict_samples_per_second      =    171.312
  predict_steps_per_second        =      0.671
[2026-02-26 00:16:45,551] [INFO] [launch.py:347:main] Process 1583857 exits successfully.
[2026-02-26 00:16:46,551] [INFO] [launch.py:347:main] Process 1583858 exits successfully.
Finished.
