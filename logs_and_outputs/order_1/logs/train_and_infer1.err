+ set -x
+ export CUDA_DEVICE_ORDER=PCI_BUS_ID
+ CUDA_DEVICE_ORDER=PCI_BUS_ID
+ export HF_HOME=/home/yangye/.cache/huggingface
+ HF_HOME=/home/yangye/.cache/huggingface
+ export TRANSFORMERS_CACHE=/home/yangye/.cache/huggingface
+ TRANSFORMERS_CACHE=/home/yangye/.cache/huggingface
+ mkdir -p /home/yangye/.cache/huggingface
++ shuf -i25000-30000 -n1
+ port=27668
+ python src/run_uie_lora.py --do_train --do_predict --predict_with_generate --model_name_or_path /workspace/data/MLLMs/yangye/initial_model/t5-large --data_dir CL_Benchmark --task_config_dir configs/order1_configs/dbpedia --instruction_file configs/instruction_config.json --instruction_strategy single --output_dir logs_and_outputs/order_1/outputs/1-dbpedia --per_device_train_batch_size 32 --per_device_eval_batch_size 128 --gradient_accumulation_steps 2 --learning_rate 1e-03 --num_train_epochs 1 --run_name order1_round1 --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --evaluation_strategy no --save_strategy no --save_steps 1500 --lamda_1 0 --lamda_2 0
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/accelerate/utils/torch_xla.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 224.08it/s]
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/nn/modules/linear.py:124: UserWarning: Initializing zero-element tensors is a no-op
  init.kaiming_uniform_(self.weight, a=math.sqrt(5))
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/219 [00:00<?, ?it/s]/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Traceback (most recent call last):
  File "/home/yangye/O-LoRA/src/run_uie_lora.py", line 587, in <module>
    main()
  File "/home/yangye/O-LoRA/src/run_uie_lora.py", line 527, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/trainer.py", line 1929, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yangye/O-LoRA/src/uie_trainer_lora.py", line 81, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/trainer.py", line 2731, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yangye/O-LoRA/src/peft/peft_model.py", line 869, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 1716, in forward
    decoder_outputs = self.decoder(
                      ^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 1086, in forward
    layer_outputs = layer_module(
                    ^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 723, in forward
    cross_attention_outputs = self.layer[1](
                              ^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 634, in forward
    attention_output = self.EncDecAttention(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 525, in forward
    value_states = project(
                   ^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 500, in project
    hidden_states = shape(proj_layer(key_value_states))
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yangye/O-LoRA/src/peft/tuners/lora.py", line 586, in forward
    self.lora_B[self.active_adapter](
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.37 GiB memory in use. Of the allocated memory 35.39 GiB is allocated by PyTorch, and 3.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/219 [00:02<?, ?it/s]
