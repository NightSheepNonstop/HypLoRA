+ set -x
+ export CUDA_DEVICE_ORDER=PCI_BUS_ID
+ CUDA_DEVICE_ORDER=PCI_BUS_ID
+ export HF_HOME=/home/yangye/.cache/huggingface
+ HF_HOME=/home/yangye/.cache/huggingface
+ export TRANSFORMERS_CACHE=/home/yangye/.cache/huggingface
+ TRANSFORMERS_CACHE=/home/yangye/.cache/huggingface
+ mkdir -p /home/yangye/.cache/huggingface
++ shuf -i25000-30000 -n1
+ port=28536
+ deepspeed --num_gpus=2 --master_port 28536 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --model_name_or_path /workspace/data/MLLMs/yangye/initial_model/t5-large --data_dir CL_Benchmark --task_config_dir configs/order1_configs/dbpedia --instruction_file configs/instruction_config.json --instruction_strategy single --output_dir logs_and_outputs/order_1/outputs/1-dbpedia --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --num_train_epochs 1 --deepspeed configs/ds_configs/stage2.config --run_name order1_round1 --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --evaluation_strategy no --save_strategy no --save_steps 1500 --lamda_1 0.5 --lamda_2 0
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/accelerate/utils/torch_xla.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/accelerate/utils/torch_xla.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/accelerate/utils/torch_xla.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/accelerate/utils/torch_xla.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 862.02it/s]
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 910.68it/s]
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/nn/modules/linear.py:124: UserWarning: Initializing zero-element tensors is a no-op
  init.kaiming_uniform_(self.weight, a=math.sqrt(5))
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/nn/modules/linear.py:124: UserWarning: Initializing zero-element tensors is a no-op
  init.kaiming_uniform_(self.weight, a=math.sqrt(5))
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  return func(*args, **kwargs)
[rank0]:[W225 23:47:05.645074773 ProcessGroupNCCL.cpp:5138] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
  0%|          | 0/218 [00:00<?, ?it/s]/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/autograd/graph.py:865: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:76.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/autograd/graph.py:865: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:76.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1830: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1830: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  0%|          | 1/218 [00:02<09:19,  2.58s/it]  1%|          | 2/218 [00:04<08:40,  2.41s/it]  1%|▏         | 3/218 [00:07<08:27,  2.36s/it]  2%|▏         | 4/218 [00:09<08:21,  2.34s/it]  2%|▏         | 5/218 [00:11<08:16,  2.33s/it]  3%|▎         | 6/218 [00:14<08:12,  2.32s/it]  3%|▎         | 7/218 [00:16<08:08,  2.31s/it]  4%|▎         | 8/218 [00:18<08:04,  2.31s/it]  4%|▍         | 9/218 [00:21<08:03,  2.31s/it]  5%|▍         | 10/218 [00:23<08:00,  2.31s/it]                                                  5%|▍         | 10/218 [00:23<08:00,  2.31s/it]  5%|▌         | 11/218 [00:25<07:58,  2.31s/it]  6%|▌         | 12/218 [00:27<07:55,  2.31s/it]  6%|▌         | 13/218 [00:30<07:54,  2.32s/it]  6%|▋         | 14/218 [00:32<07:51,  2.31s/it]  7%|▋         | 15/218 [00:34<07:50,  2.32s/it]  7%|▋         | 16/218 [00:37<07:47,  2.32s/it]  8%|▊         | 17/218 [00:39<07:44,  2.31s/it]  8%|▊         | 18/218 [00:41<07:42,  2.31s/it]  9%|▊         | 19/218 [00:44<07:39,  2.31s/it]  9%|▉         | 20/218 [00:46<07:37,  2.31s/it]                                                  9%|▉         | 20/218 [00:46<07:37,  2.31s/it] 10%|▉         | 21/218 [00:48<07:35,  2.31s/it] 10%|█         | 22/218 [00:51<07:33,  2.31s/it] 11%|█         | 23/218 [00:53<07:35,  2.34s/it] 11%|█         | 24/218 [00:55<07:31,  2.33s/it] 11%|█▏        | 25/218 [00:58<07:28,  2.32s/it] 12%|█▏        | 26/218 [01:00<07:25,  2.32s/it] 12%|█▏        | 27/218 [01:02<07:22,  2.32s/it] 13%|█▎        | 28/218 [01:05<07:20,  2.32s/it] 13%|█▎        | 29/218 [01:07<07:19,  2.33s/it] 14%|█▍        | 30/218 [01:09<07:16,  2.32s/it]                                                 14%|█▍        | 30/218 [01:09<07:16,  2.32s/it] 14%|█▍        | 31/218 [01:12<07:14,  2.32s/it] 15%|█▍        | 32/218 [01:14<07:12,  2.32s/it] 15%|█▌        | 33/218 [01:16<07:09,  2.32s/it] 16%|█▌        | 34/218 [01:18<07:07,  2.32s/it] 16%|█▌        | 35/218 [01:21<07:05,  2.32s/it] 17%|█▋        | 36/218 [01:23<07:03,  2.33s/it] 17%|█▋        | 37/218 [01:25<07:00,  2.32s/it] 17%|█▋        | 38/218 [01:28<06:58,  2.32s/it] 18%|█▊        | 39/218 [01:30<06:55,  2.32s/it] 18%|█▊        | 40/218 [01:32<06:53,  2.32s/it]                                                 18%|█▊        | 40/218 [01:32<06:53,  2.32s/it] 19%|█▉        | 41/218 [01:35<06:51,  2.32s/it] 19%|█▉        | 42/218 [01:37<06:49,  2.33s/it] 20%|█▉        | 43/218 [01:39<06:46,  2.32s/it] 20%|██        | 44/218 [01:42<06:44,  2.32s/it] 21%|██        | 45/218 [01:44<06:41,  2.32s/it] 21%|██        | 46/218 [01:46<06:39,  2.33s/it] 22%|██▏       | 47/218 [01:49<06:37,  2.32s/it] 22%|██▏       | 48/218 [01:51<06:34,  2.32s/it] 22%|██▏       | 49/218 [01:53<06:32,  2.32s/it] 23%|██▎       | 50/218 [01:56<06:30,  2.32s/it]                                                 23%|██▎       | 50/218 [01:56<06:30,  2.32s/it] 23%|██▎       | 51/218 [01:58<06:27,  2.32s/it] 24%|██▍       | 52/218 [02:00<06:25,  2.32s/it] 24%|██▍       | 53/218 [02:03<06:23,  2.33s/it] 25%|██▍       | 54/218 [02:05<06:21,  2.33s/it] 25%|██▌       | 55/218 [02:07<06:18,  2.32s/it] 26%|██▌       | 56/218 [02:10<06:16,  2.32s/it] 26%|██▌       | 57/218 [02:12<06:13,  2.32s/it] 27%|██▋       | 58/218 [02:14<06:11,  2.32s/it] 27%|██▋       | 59/218 [02:17<06:08,  2.32s/it] 28%|██▊       | 60/218 [02:19<06:06,  2.32s/it]                                                 28%|██▊       | 60/218 [02:19<06:06,  2.32s/it] 28%|██▊       | 61/218 [02:21<06:04,  2.32s/it] 28%|██▊       | 62/218 [02:24<06:02,  2.32s/it] 29%|██▉       | 63/218 [02:26<05:59,  2.32s/it] 29%|██▉       | 64/218 [02:28<05:57,  2.32s/it] 30%|██▉       | 65/218 [02:30<05:54,  2.32s/it] 30%|███       | 66/218 [02:33<05:52,  2.32s/it] 31%|███       | 67/218 [02:35<05:50,  2.32s/it] 31%|███       | 68/218 [02:37<05:48,  2.32s/it] 32%|███▏      | 69/218 [02:40<05:45,  2.32s/it] 32%|███▏      | 70/218 [02:42<05:43,  2.32s/it]                                                 32%|███▏      | 70/218 [02:42<05:43,  2.32s/it] 33%|███▎      | 71/218 [02:44<05:41,  2.32s/it] 33%|███▎      | 72/218 [02:47<05:39,  2.32s/it] 33%|███▎      | 73/218 [02:49<05:36,  2.32s/it] 34%|███▍      | 74/218 [02:51<05:34,  2.33s/it] 34%|███▍      | 75/218 [02:54<05:32,  2.32s/it] 35%|███▍      | 76/218 [02:56<05:29,  2.32s/it] 35%|███▌      | 77/218 [02:58<05:27,  2.32s/it] 36%|███▌      | 78/218 [03:01<05:25,  2.32s/it] 36%|███▌      | 79/218 [03:03<05:22,  2.32s/it] 37%|███▋      | 80/218 [03:05<05:20,  2.32s/it]                                                 37%|███▋      | 80/218 [03:05<05:20,  2.32s/it] 37%|███▋      | 81/218 [03:08<05:18,  2.32s/it] 38%|███▊      | 82/218 [03:10<05:16,  2.33s/it] 38%|███▊      | 83/218 [03:12<05:14,  2.33s/it] 39%|███▊      | 84/218 [03:15<05:11,  2.32s/it] 39%|███▉      | 85/218 [03:17<05:09,  2.33s/it] 39%|███▉      | 86/218 [03:19<05:06,  2.32s/it] 40%|███▉      | 87/218 [03:22<05:04,  2.32s/it] 40%|████      | 88/218 [03:24<05:01,  2.32s/it] 41%|████      | 89/218 [03:26<04:58,  2.32s/it] 41%|████▏     | 90/218 [03:29<04:56,  2.32s/it]                                                 41%|████▏     | 90/218 [03:29<04:56,  2.32s/it] 42%|████▏     | 91/218 [03:31<04:54,  2.32s/it] 42%|████▏     | 92/218 [03:33<04:51,  2.31s/it] 43%|████▎     | 93/218 [03:35<04:48,  2.31s/it] 43%|████▎     | 94/218 [03:38<04:47,  2.32s/it] 44%|████▎     | 95/218 [03:40<04:44,  2.31s/it] 44%|████▍     | 96/218 [03:42<04:42,  2.31s/it] 44%|████▍     | 97/218 [03:45<04:40,  2.32s/it] 45%|████▍     | 98/218 [03:47<04:38,  2.32s/it] 45%|████▌     | 99/218 [03:49<04:36,  2.32s/it] 46%|████▌     | 100/218 [03:52<04:35,  2.33s/it]                                                  46%|████▌     | 100/218 [03:52<04:35,  2.33s/it] 46%|████▋     | 101/218 [03:54<04:33,  2.34s/it] 47%|████▋     | 102/218 [03:56<04:30,  2.34s/it] 47%|████▋     | 103/218 [03:59<04:27,  2.33s/it] 48%|████▊     | 104/218 [04:01<04:25,  2.33s/it] 48%|████▊     | 105/218 [04:03<04:22,  2.32s/it] 49%|████▊     | 106/218 [04:06<04:20,  2.33s/it] 49%|████▉     | 107/218 [04:08<04:17,  2.32s/it] 50%|████▉     | 108/218 [04:10<04:15,  2.32s/it] 50%|█████     | 109/218 [04:13<04:12,  2.32s/it] 50%|█████     | 110/218 [04:15<04:10,  2.32s/it]                                                  50%|█████     | 110/218 [04:15<04:10,  2.32s/it] 51%|█████     | 111/218 [04:17<04:08,  2.32s/it] 51%|█████▏    | 112/218 [04:20<04:06,  2.32s/it] 52%|█████▏    | 113/218 [04:22<04:04,  2.32s/it] 52%|█████▏    | 114/218 [04:24<04:01,  2.32s/it] 53%|█████▎    | 115/218 [04:27<03:58,  2.32s/it] 53%|█████▎    | 116/218 [04:29<03:56,  2.32s/it] 54%|█████▎    | 117/218 [04:31<03:53,  2.31s/it] 54%|█████▍    | 118/218 [04:34<03:51,  2.32s/it] 55%|█████▍    | 119/218 [04:36<03:49,  2.32s/it] 55%|█████▌    | 120/218 [04:38<03:46,  2.32s/it]                                                  55%|█████▌    | 120/218 [04:38<03:46,  2.32s/it] 56%|█████▌    | 121/218 [04:41<03:45,  2.32s/it] 56%|█████▌    | 122/218 [04:43<03:42,  2.32s/it] 56%|█████▋    | 123/218 [04:45<03:40,  2.33s/it] 57%|█████▋    | 124/218 [04:47<03:38,  2.32s/it] 57%|█████▋    | 125/218 [04:50<03:36,  2.32s/it] 58%|█████▊    | 126/218 [04:52<03:34,  2.33s/it] 58%|█████▊    | 127/218 [04:54<03:32,  2.33s/it] 59%|█████▊    | 128/218 [04:57<03:29,  2.33s/it] 59%|█████▉    | 129/218 [04:59<03:27,  2.33s/it] 60%|█████▉    | 130/218 [05:01<03:24,  2.32s/it]                                                  60%|█████▉    | 130/218 [05:01<03:24,  2.32s/it] 60%|██████    | 131/218 [05:04<03:23,  2.34s/it] 61%|██████    | 132/218 [05:06<03:20,  2.33s/it] 61%|██████    | 133/218 [05:08<03:18,  2.33s/it] 61%|██████▏   | 134/218 [05:11<03:15,  2.33s/it] 62%|██████▏   | 135/218 [05:13<03:13,  2.33s/it] 62%|██████▏   | 136/218 [05:15<03:11,  2.33s/it] 63%|██████▎   | 137/218 [05:18<03:08,  2.33s/it] 63%|██████▎   | 138/218 [05:20<03:05,  2.32s/it] 64%|██████▍   | 139/218 [05:22<03:03,  2.32s/it] 64%|██████▍   | 140/218 [05:25<03:01,  2.32s/it]                                                  64%|██████▍   | 140/218 [05:25<03:01,  2.32s/it] 65%|██████▍   | 141/218 [05:27<02:58,  2.32s/it] 65%|██████▌   | 142/218 [05:29<02:56,  2.32s/it] 66%|██████▌   | 143/218 [05:32<02:53,  2.32s/it] 66%|██████▌   | 144/218 [05:34<02:51,  2.32s/it] 67%|██████▋   | 145/218 [05:36<02:49,  2.32s/it] 67%|██████▋   | 146/218 [05:39<02:47,  2.33s/it] 67%|██████▋   | 147/218 [05:41<02:45,  2.33s/it] 68%|██████▊   | 148/218 [05:43<02:42,  2.32s/it] 68%|██████▊   | 149/218 [05:46<02:40,  2.32s/it] 69%|██████▉   | 150/218 [05:48<02:37,  2.32s/it]                                                  69%|██████▉   | 150/218 [05:48<02:37,  2.32s/it] 69%|██████▉   | 151/218 [05:50<02:35,  2.32s/it] 70%|██████▉   | 152/218 [05:53<02:33,  2.32s/it] 70%|███████   | 153/218 [05:55<02:31,  2.32s/it] 71%|███████   | 154/218 [05:57<02:28,  2.32s/it] 71%|███████   | 155/218 [06:00<02:26,  2.32s/it] 72%|███████▏  | 156/218 [06:02<02:23,  2.32s/it] 72%|███████▏  | 157/218 [06:04<02:21,  2.32s/it] 72%|███████▏  | 158/218 [06:06<02:18,  2.31s/it] 73%|███████▎  | 159/218 [06:09<02:16,  2.31s/it] 73%|███████▎  | 160/218 [06:11<02:14,  2.32s/it]                                                  73%|███████▎  | 160/218 [06:11<02:14,  2.32s/it] 74%|███████▍  | 161/218 [06:13<02:12,  2.32s/it] 74%|███████▍  | 162/218 [06:16<02:09,  2.31s/it] 75%|███████▍  | 163/218 [06:18<02:07,  2.32s/it] 75%|███████▌  | 164/218 [06:20<02:05,  2.32s/it] 76%|███████▌  | 165/218 [06:23<02:03,  2.32s/it] 76%|███████▌  | 166/218 [06:25<02:00,  2.32s/it] 77%|███████▋  | 167/218 [06:27<01:58,  2.32s/it] 77%|███████▋  | 168/218 [06:30<01:56,  2.32s/it] 78%|███████▊  | 169/218 [06:32<01:53,  2.32s/it] 78%|███████▊  | 170/218 [06:34<01:51,  2.32s/it]                                                  78%|███████▊  | 170/218 [06:34<01:51,  2.32s/it] 78%|███████▊  | 171/218 [06:37<01:49,  2.32s/it] 79%|███████▉  | 172/218 [06:39<01:46,  2.32s/it] 79%|███████▉  | 173/218 [06:41<01:44,  2.32s/it] 80%|███████▉  | 174/218 [06:44<01:42,  2.32s/it] 80%|████████  | 175/218 [06:46<01:39,  2.32s/it] 81%|████████  | 176/218 [06:48<01:37,  2.32s/it] 81%|████████  | 177/218 [06:51<01:35,  2.33s/it] 82%|████████▏ | 178/218 [06:53<01:32,  2.32s/it] 82%|████████▏ | 179/218 [06:55<01:30,  2.32s/it] 83%|████████▎ | 180/218 [06:58<01:28,  2.32s/it]                                                  83%|████████▎ | 180/218 [06:58<01:28,  2.32s/it] 83%|████████▎ | 181/218 [07:00<01:25,  2.32s/it] 83%|████████▎ | 182/218 [07:02<01:23,  2.32s/it] 84%|████████▍ | 183/218 [07:05<01:21,  2.32s/it] 84%|████████▍ | 184/218 [07:07<01:18,  2.32s/it] 85%|████████▍ | 185/218 [07:09<01:16,  2.32s/it] 85%|████████▌ | 186/218 [07:11<01:14,  2.32s/it] 86%|████████▌ | 187/218 [07:14<01:11,  2.32s/it] 86%|████████▌ | 188/218 [07:16<01:09,  2.32s/it] 87%|████████▋ | 189/218 [07:18<01:07,  2.31s/it] 87%|████████▋ | 190/218 [07:21<01:04,  2.32s/it]                                                  87%|████████▋ | 190/218 [07:21<01:04,  2.32s/it] 88%|████████▊ | 191/218 [07:23<01:02,  2.32s/it] 88%|████████▊ | 192/218 [07:25<01:00,  2.33s/it] 89%|████████▊ | 193/218 [07:28<00:58,  2.32s/it] 89%|████████▉ | 194/218 [07:30<00:55,  2.32s/it] 89%|████████▉ | 195/218 [07:32<00:53,  2.32s/it] 90%|████████▉ | 196/218 [07:35<00:51,  2.33s/it] 90%|█████████ | 197/218 [07:37<00:48,  2.32s/it] 91%|█████████ | 198/218 [07:39<00:46,  2.31s/it] 91%|█████████▏| 199/218 [07:42<00:43,  2.31s/it] 92%|█████████▏| 200/218 [07:44<00:41,  2.31s/it]                                                  92%|█████████▏| 200/218 [07:44<00:41,  2.31s/it] 92%|█████████▏| 201/218 [07:46<00:39,  2.32s/it] 93%|█████████▎| 202/218 [07:49<00:36,  2.31s/it] 93%|█████████▎| 203/218 [07:51<00:34,  2.31s/it] 94%|█████████▎| 204/218 [07:53<00:32,  2.31s/it] 94%|█████████▍| 205/218 [07:55<00:29,  2.31s/it] 94%|█████████▍| 206/218 [07:58<00:27,  2.31s/it] 95%|█████████▍| 207/218 [08:00<00:25,  2.32s/it] 95%|█████████▌| 208/218 [08:02<00:23,  2.31s/it] 96%|█████████▌| 209/218 [08:05<00:20,  2.32s/it] 96%|█████████▋| 210/218 [08:07<00:18,  2.32s/it]                                                  96%|█████████▋| 210/218 [08:07<00:18,  2.32s/it] 97%|█████████▋| 211/218 [08:09<00:16,  2.32s/it] 97%|█████████▋| 212/218 [08:12<00:13,  2.32s/it] 98%|█████████▊| 213/218 [08:14<00:11,  2.32s/it] 98%|█████████▊| 214/218 [08:16<00:09,  2.32s/it] 99%|█████████▊| 215/218 [08:19<00:06,  2.32s/it] 99%|█████████▉| 216/218 [08:21<00:04,  2.32s/it]100%|█████████▉| 217/218 [08:23<00:02,  2.32s/it]100%|██████████| 218/218 [08:26<00:00,  2.32s/it]                                                 100%|██████████| 218/218 [08:26<00:00,  2.32s/it]100%|██████████| 218/218 [08:26<00:00,  2.32s/it]
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:12,  2.19it/s] 10%|█         | 3/30 [00:01<00:17,  1.56it/s] 13%|█▎        | 4/30 [00:02<00:19,  1.34it/s] 17%|█▋        | 5/30 [00:03<00:19,  1.26it/s] 20%|██        | 6/30 [00:04<00:19,  1.20it/s] 23%|██▎       | 7/30 [00:05<00:19,  1.19it/s] 27%|██▋       | 8/30 [00:06<00:18,  1.19it/s] 30%|███       | 9/30 [00:07<00:18,  1.15it/s] 33%|███▎      | 10/30 [00:07<00:17,  1.17it/s] 37%|███▋      | 11/30 [00:08<00:16,  1.14it/s] 40%|████      | 12/30 [00:09<00:16,  1.12it/s] 43%|████▎     | 13/30 [00:11<00:20,  1.22s/it] 47%|████▋     | 14/30 [00:12<00:17,  1.11s/it] 50%|█████     | 15/30 [00:13<00:16,  1.11s/it] 53%|█████▎    | 16/30 [00:14<00:14,  1.03s/it] 57%|█████▋    | 17/30 [00:15<00:12,  1.02it/s] 60%|██████    | 18/30 [00:16<00:11,  1.06it/s] 63%|██████▎   | 19/30 [00:17<00:10,  1.10it/s] 67%|██████▋   | 20/30 [00:18<00:09,  1.11it/s] 70%|███████   | 21/30 [00:18<00:08,  1.12it/s] 73%|███████▎  | 22/30 [00:20<00:09,  1.21s/it] 77%|███████▋  | 23/30 [00:21<00:07,  1.12s/it] 80%|████████  | 24/30 [00:22<00:06,  1.11s/it] 83%|████████▎ | 25/30 [00:24<00:05,  1.12s/it] 87%|████████▋ | 26/30 [00:25<00:04,  1.09s/it] 90%|█████████ | 27/30 [00:25<00:03,  1.05s/it] 93%|█████████▎| 28/30 [00:26<00:02,  1.04s/it] 97%|█████████▋| 29/30 [00:27<00:00,  1.01it/s]100%|██████████| 30/30 [00:29<00:00,  1.04s/it]100%|██████████| 30/30 [00:32<00:00,  1.10s/it]
[rank0]:[W225 23:56:07.870025654 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
+ sleep 5
+ deepspeed --num_gpus=2 --master_port 28536 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --model_name_or_path logs_and_outputs/order_1/outputs/1-dbpedia/adapter --data_dir CL_Benchmark --task_config_dir configs/order1_configs/amazon --instruction_file configs/instruction_config.json --instruction_strategy single --output_dir logs_and_outputs/order_1/outputs/2-amazon --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --num_train_epochs 1 --deepspeed configs/ds_configs/stage2.config --run_name order1_round2 --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --evaluation_strategy no --save_strategy no --save_steps 1500 --lamda_1 0.5 --lamda_2 0
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/accelerate/utils/torch_xla.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/accelerate/utils/torch_xla.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/accelerate/utils/torch_xla.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/accelerate/utils/torch_xla.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
  0%|          | 0/3 [00:00<?, ?it/s]Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
100%|██████████| 3/3 [00:00<00:00, 801.05it/s]
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 848.48it/s]
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  return func(*args, **kwargs)
[rank0]:[W225 23:56:44.593888855 ProcessGroupNCCL.cpp:5138] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
  0%|          | 0/78 [00:00<?, ?it/s]/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/autograd/graph.py:865: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:76.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/autograd/graph.py:865: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:76.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1830: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1830: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  1%|▏         | 1/78 [00:02<03:35,  2.79s/it]  3%|▎         | 2/78 [00:05<03:16,  2.58s/it]  4%|▍         | 3/78 [00:07<03:07,  2.50s/it]  5%|▌         | 4/78 [00:10<03:01,  2.46s/it]  6%|▋         | 5/78 [00:12<02:57,  2.43s/it]  8%|▊         | 6/78 [00:14<02:54,  2.42s/it]  9%|▉         | 7/78 [00:17<02:51,  2.41s/it] 10%|█         | 8/78 [00:19<02:48,  2.40s/it] 12%|█▏        | 9/78 [00:22<02:46,  2.41s/it] 13%|█▎        | 10/78 [00:24<02:43,  2.40s/it]                                                13%|█▎        | 10/78 [00:24<02:43,  2.40s/it] 14%|█▍        | 11/78 [00:26<02:41,  2.40s/it] 15%|█▌        | 12/78 [00:29<02:38,  2.40s/it] 17%|█▋        | 13/78 [00:31<02:36,  2.40s/it] 18%|█▊        | 14/78 [00:33<02:33,  2.40s/it] 19%|█▉        | 15/78 [00:36<02:30,  2.39s/it] 21%|██        | 16/78 [00:38<02:28,  2.39s/it] 22%|██▏       | 17/78 [00:41<02:25,  2.39s/it] 23%|██▎       | 18/78 [00:43<02:23,  2.39s/it] 24%|██▍       | 19/78 [00:45<02:21,  2.40s/it] 26%|██▌       | 20/78 [00:48<02:19,  2.40s/it]                                                26%|██▌       | 20/78 [00:48<02:19,  2.40s/it] 27%|██▋       | 21/78 [00:50<02:16,  2.40s/it] 28%|██▊       | 22/78 [00:53<02:14,  2.39s/it] 29%|██▉       | 23/78 [00:55<02:11,  2.39s/it] 31%|███       | 24/78 [00:57<02:09,  2.39s/it] 32%|███▏      | 25/78 [01:00<02:06,  2.39s/it] 33%|███▎      | 26/78 [01:02<02:04,  2.39s/it] 35%|███▍      | 27/78 [01:05<02:02,  2.39s/it] 36%|███▌      | 28/78 [01:07<01:59,  2.39s/it] 37%|███▋      | 29/78 [01:09<01:57,  2.40s/it] 38%|███▊      | 30/78 [01:12<01:55,  2.40s/it]                                                38%|███▊      | 30/78 [01:12<01:55,  2.40s/it] 40%|███▉      | 31/78 [01:14<01:52,  2.40s/it] 41%|████      | 32/78 [01:17<01:50,  2.39s/it] 42%|████▏     | 33/78 [01:19<01:47,  2.39s/it] 44%|████▎     | 34/78 [01:21<01:45,  2.40s/it] 45%|████▍     | 35/78 [01:24<01:42,  2.39s/it] 46%|████▌     | 36/78 [01:26<01:40,  2.39s/it] 47%|████▋     | 37/78 [01:29<01:38,  2.40s/it] 49%|████▊     | 38/78 [01:31<01:35,  2.39s/it] 50%|█████     | 39/78 [01:33<01:33,  2.40s/it] 51%|█████▏    | 40/78 [01:36<01:31,  2.39s/it]                                                51%|█████▏    | 40/78 [01:36<01:31,  2.39s/it] 53%|█████▎    | 41/78 [01:38<01:28,  2.40s/it] 54%|█████▍    | 42/78 [01:41<01:26,  2.40s/it] 55%|█████▌    | 43/78 [01:43<01:23,  2.40s/it] 56%|█████▋    | 44/78 [01:45<01:21,  2.40s/it] 58%|█████▊    | 45/78 [01:48<01:19,  2.39s/it] 59%|█████▉    | 46/78 [01:50<01:16,  2.40s/it] 60%|██████    | 47/78 [01:53<01:14,  2.40s/it] 62%|██████▏   | 48/78 [01:55<01:11,  2.40s/it] 63%|██████▎   | 49/78 [01:57<01:09,  2.40s/it] 64%|██████▍   | 50/78 [02:00<01:06,  2.39s/it]                                                64%|██████▍   | 50/78 [02:00<01:06,  2.39s/it] 65%|██████▌   | 51/78 [02:02<01:04,  2.39s/it] 67%|██████▋   | 52/78 [02:04<01:02,  2.40s/it] 68%|██████▊   | 53/78 [02:07<00:59,  2.39s/it] 69%|██████▉   | 54/78 [02:09<00:57,  2.39s/it] 71%|███████   | 55/78 [02:12<00:54,  2.39s/it] 72%|███████▏  | 56/78 [02:14<00:52,  2.39s/it] 73%|███████▎  | 57/78 [02:16<00:50,  2.39s/it] 74%|███████▍  | 58/78 [02:19<00:47,  2.39s/it] 76%|███████▌  | 59/78 [02:21<00:45,  2.39s/it] 77%|███████▋  | 60/78 [02:24<00:42,  2.38s/it]                                                77%|███████▋  | 60/78 [02:24<00:42,  2.38s/it] 78%|███████▊  | 61/78 [02:26<00:40,  2.39s/it] 79%|███████▉  | 62/78 [02:28<00:38,  2.40s/it] 81%|████████  | 63/78 [02:31<00:35,  2.40s/it] 82%|████████▏ | 64/78 [02:33<00:33,  2.40s/it] 83%|████████▎ | 65/78 [02:36<00:31,  2.40s/it] 85%|████████▍ | 66/78 [02:38<00:28,  2.40s/it] 86%|████████▌ | 67/78 [02:40<00:26,  2.40s/it] 87%|████████▋ | 68/78 [02:43<00:24,  2.40s/it] 88%|████████▊ | 69/78 [02:45<00:21,  2.40s/it] 90%|████████▉ | 70/78 [02:48<00:19,  2.41s/it]                                                90%|████████▉ | 70/78 [02:48<00:19,  2.41s/it] 91%|█████████ | 71/78 [02:50<00:16,  2.41s/it] 92%|█████████▏| 72/78 [02:52<00:14,  2.41s/it] 94%|█████████▎| 73/78 [02:55<00:12,  2.41s/it] 95%|█████████▍| 74/78 [02:57<00:09,  2.40s/it] 96%|█████████▌| 75/78 [03:00<00:07,  2.40s/it] 97%|█████████▋| 76/78 [03:02<00:04,  2.39s/it] 99%|█████████▊| 77/78 [03:04<00:02,  2.40s/it]100%|██████████| 78/78 [03:07<00:00,  2.40s/it]                                               100%|██████████| 78/78 [03:07<00:00,  2.40s/it]100%|██████████| 78/78 [03:07<00:00,  2.40s/it]
  0%|          | 0/60 [00:00<?, ?it/s]  3%|▎         | 2/60 [00:01<00:35,  1.66it/s]  5%|▌         | 3/60 [00:02<00:52,  1.10it/s]  7%|▋         | 4/60 [00:03<00:57,  1.03s/it]  8%|▊         | 5/60 [00:04<01:00,  1.09s/it] 10%|█         | 6/60 [00:06<01:01,  1.14s/it] 12%|█▏        | 7/60 [00:07<00:59,  1.13s/it] 13%|█▎        | 8/60 [00:08<00:59,  1.15s/it] 15%|█▌        | 9/60 [00:10<01:05,  1.28s/it] 17%|█▋        | 10/60 [00:11<01:07,  1.34s/it] 18%|█▊        | 11/60 [00:12<01:02,  1.27s/it] 20%|██        | 12/60 [00:13<00:59,  1.24s/it] 22%|██▏       | 13/60 [00:15<00:58,  1.24s/it] 23%|██▎       | 14/60 [00:16<00:57,  1.26s/it] 25%|██▌       | 15/60 [00:17<01:00,  1.35s/it] 27%|██▋       | 16/60 [00:19<00:56,  1.28s/it] 28%|██▊       | 17/60 [00:20<00:53,  1.23s/it] 30%|███       | 18/60 [00:21<00:51,  1.22s/it] 32%|███▏      | 19/60 [00:22<00:49,  1.20s/it] 33%|███▎      | 20/60 [00:23<00:47,  1.18s/it] 35%|███▌      | 21/60 [00:24<00:45,  1.17s/it] 37%|███▋      | 22/60 [00:25<00:44,  1.17s/it] 38%|███▊      | 23/60 [00:27<00:46,  1.26s/it] 40%|████      | 24/60 [00:28<00:46,  1.28s/it] 42%|████▏     | 25/60 [00:29<00:43,  1.25s/it] 43%|████▎     | 26/60 [00:31<00:41,  1.22s/it] 45%|████▌     | 27/60 [00:32<00:39,  1.20s/it] 47%|████▋     | 28/60 [00:33<00:36,  1.16s/it] 48%|████▊     | 29/60 [00:34<00:36,  1.18s/it] 50%|█████     | 30/60 [00:35<00:35,  1.19s/it] 52%|█████▏    | 31/60 [00:36<00:34,  1.18s/it] 53%|█████▎    | 32/60 [00:37<00:31,  1.11s/it] 55%|█████▌    | 33/60 [00:38<00:28,  1.07s/it] 57%|█████▋    | 34/60 [00:39<00:26,  1.02s/it] 58%|█████▊    | 35/60 [00:40<00:24,  1.00it/s] 60%|██████    | 36/60 [00:41<00:23,  1.03it/s] 62%|██████▏   | 37/60 [00:42<00:21,  1.07it/s] 63%|██████▎   | 38/60 [00:43<00:20,  1.05it/s] 65%|██████▌   | 39/60 [00:44<00:19,  1.07it/s] 67%|██████▋   | 40/60 [00:45<00:18,  1.05it/s] 68%|██████▊   | 41/60 [00:46<00:17,  1.06it/s] 70%|███████   | 42/60 [00:47<00:17,  1.04it/s] 72%|███████▏  | 43/60 [00:49<00:21,  1.27s/it] 73%|███████▎  | 44/60 [00:50<00:19,  1.24s/it] 75%|███████▌  | 45/60 [00:51<00:17,  1.14s/it] 77%|███████▋  | 46/60 [00:52<00:14,  1.07s/it] 78%|███████▊  | 47/60 [00:53<00:13,  1.02s/it] 80%|████████  | 48/60 [00:54<00:11,  1.02it/s] 82%|████████▏ | 49/60 [00:54<00:10,  1.04it/s] 83%|████████▎ | 50/60 [00:55<00:09,  1.05it/s] 85%|████████▌ | 51/60 [00:57<00:11,  1.27s/it] 87%|████████▋ | 52/60 [00:58<00:09,  1.17s/it] 88%|████████▊ | 53/60 [00:59<00:07,  1.09s/it] 90%|█████████ | 54/60 [01:00<00:06,  1.09s/it] 92%|█████████▏| 55/60 [01:01<00:05,  1.12s/it] 93%|█████████▎| 56/60 [01:02<00:04,  1.08s/it] 95%|█████████▌| 57/60 [01:03<00:03,  1.04s/it] 97%|█████████▋| 58/60 [01:05<00:02,  1.05s/it] 98%|█████████▊| 59/60 [01:06<00:01,  1.03s/it]100%|██████████| 60/60 [01:07<00:00,  1.05s/it]100%|██████████| 60/60 [01:15<00:00,  1.25s/it]
[rank0]:[W226 00:01:09.138214848 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
+ sleep 5
+ deepspeed --num_gpus=2 --master_port 28536 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --model_name_or_path logs_and_outputs/order_1/outputs/2-amazon/adapter --data_dir CL_Benchmark --task_config_dir configs/order1_configs/yahoo --instruction_file configs/instruction_config.json --instruction_strategy single --output_dir logs_and_outputs/order_1/outputs/3-yahoo --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --num_train_epochs 1 --deepspeed configs/ds_configs/stage2.config --run_name order1_round3 --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --evaluation_strategy no --save_strategy no --save_steps 1500 --lamda_1 0.5 --lamda_2 0
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/accelerate/utils/torch_xla.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/accelerate/utils/torch_xla.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/accelerate/utils/torch_xla.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/accelerate/utils/torch_xla.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 858.37it/s]
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 878.76it/s]
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  return func(*args, **kwargs)
[rank0]:[W226 00:01:47.066727216 ProcessGroupNCCL.cpp:5138] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
  0%|          | 0/156 [00:00<?, ?it/s]/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/autograd/graph.py:865: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:76.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/autograd/graph.py:865: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:76.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1830: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1830: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  1%|          | 1/156 [00:02<07:17,  2.82s/it]  1%|▏         | 2/156 [00:05<06:41,  2.60s/it]  2%|▏         | 3/156 [00:07<06:26,  2.52s/it]  3%|▎         | 4/156 [00:10<06:19,  2.50s/it]  3%|▎         | 5/156 [00:12<06:14,  2.48s/it]  4%|▍         | 6/156 [00:15<06:10,  2.47s/it]  4%|▍         | 7/156 [00:17<06:09,  2.48s/it]  5%|▌         | 8/156 [00:19<06:04,  2.46s/it]  6%|▌         | 9/156 [00:22<06:03,  2.47s/it]  6%|▋         | 10/156 [00:24<06:01,  2.48s/it]                                                  6%|▋         | 10/156 [00:24<06:01,  2.48s/it]  7%|▋         | 11/156 [00:27<06:00,  2.49s/it]  8%|▊         | 12/156 [00:29<05:54,  2.46s/it]  8%|▊         | 13/156 [00:32<05:53,  2.47s/it]  9%|▉         | 14/156 [00:34<05:52,  2.48s/it] 10%|▉         | 15/156 [00:37<05:48,  2.47s/it] 10%|█         | 16/156 [00:39<05:44,  2.46s/it] 11%|█         | 17/156 [00:42<05:41,  2.46s/it] 12%|█▏        | 18/156 [00:44<05:39,  2.46s/it] 12%|█▏        | 19/156 [00:47<05:37,  2.47s/it] 13%|█▎        | 20/156 [00:49<05:34,  2.46s/it]                                                 13%|█▎        | 20/156 [00:49<05:34,  2.46s/it] 13%|█▎        | 21/156 [00:52<05:32,  2.46s/it] 14%|█▍        | 22/156 [00:54<05:27,  2.45s/it] 15%|█▍        | 23/156 [00:56<05:26,  2.45s/it] 15%|█▌        | 24/156 [00:59<05:24,  2.46s/it] 16%|█▌        | 25/156 [01:01<05:23,  2.47s/it] 17%|█▋        | 26/156 [01:04<05:20,  2.46s/it] 17%|█▋        | 27/156 [01:06<05:17,  2.46s/it] 18%|█▊        | 28/156 [01:09<05:14,  2.46s/it] 19%|█▊        | 29/156 [01:11<05:12,  2.46s/it] 19%|█▉        | 30/156 [01:14<05:10,  2.46s/it]                                                 19%|█▉        | 30/156 [01:14<05:10,  2.46s/it] 20%|█▉        | 31/156 [01:16<05:06,  2.45s/it] 21%|██        | 32/156 [01:19<05:01,  2.43s/it] 21%|██        | 33/156 [01:21<05:00,  2.44s/it] 22%|██▏       | 34/156 [01:23<04:57,  2.44s/it] 22%|██▏       | 35/156 [01:26<04:56,  2.45s/it] 23%|██▎       | 36/156 [01:28<04:55,  2.46s/it] 24%|██▎       | 37/156 [01:31<04:52,  2.46s/it] 24%|██▍       | 38/156 [01:33<04:47,  2.44s/it] 25%|██▌       | 39/156 [01:36<04:45,  2.44s/it] 26%|██▌       | 40/156 [01:38<04:42,  2.44s/it]                                                 26%|██▌       | 40/156 [01:38<04:42,  2.44s/it] 26%|██▋       | 41/156 [01:41<04:39,  2.43s/it] 27%|██▋       | 42/156 [01:43<04:36,  2.43s/it] 28%|██▊       | 43/156 [01:45<04:36,  2.45s/it] 28%|██▊       | 44/156 [01:48<04:34,  2.45s/it] 29%|██▉       | 45/156 [01:50<04:31,  2.44s/it] 29%|██▉       | 46/156 [01:53<04:30,  2.46s/it] 30%|███       | 47/156 [01:55<04:27,  2.45s/it] 31%|███       | 48/156 [01:58<04:26,  2.47s/it] 31%|███▏      | 49/156 [02:00<04:25,  2.48s/it] 32%|███▏      | 50/156 [02:03<04:20,  2.46s/it]                                                 32%|███▏      | 50/156 [02:03<04:20,  2.46s/it] 33%|███▎      | 51/156 [02:05<04:18,  2.46s/it] 33%|███▎      | 52/156 [02:08<04:15,  2.46s/it] 34%|███▍      | 53/156 [02:10<04:13,  2.46s/it] 35%|███▍      | 54/156 [02:12<04:09,  2.45s/it] 35%|███▌      | 55/156 [02:15<04:08,  2.46s/it] 36%|███▌      | 56/156 [02:17<04:06,  2.47s/it] 37%|███▋      | 57/156 [02:20<04:03,  2.46s/it] 37%|███▋      | 58/156 [02:22<04:02,  2.47s/it] 38%|███▊      | 59/156 [02:25<03:59,  2.47s/it] 38%|███▊      | 60/156 [02:27<03:56,  2.46s/it]                                                 38%|███▊      | 60/156 [02:27<03:56,  2.46s/it] 39%|███▉      | 61/156 [02:30<03:52,  2.45s/it] 40%|███▉      | 62/156 [02:32<03:50,  2.45s/it] 40%|████      | 63/156 [02:35<03:46,  2.43s/it] 41%|████      | 64/156 [02:37<03:45,  2.45s/it] 42%|████▏     | 65/156 [02:40<03:42,  2.45s/it] 42%|████▏     | 66/156 [02:42<03:41,  2.46s/it] 43%|████▎     | 67/156 [02:44<03:38,  2.46s/it] 44%|████▎     | 68/156 [02:47<03:34,  2.44s/it] 44%|████▍     | 69/156 [02:49<03:32,  2.44s/it] 45%|████▍     | 70/156 [02:52<03:31,  2.46s/it]                                                 45%|████▍     | 70/156 [02:52<03:31,  2.46s/it] 46%|████▌     | 71/156 [02:54<03:33,  2.51s/it] 46%|████▌     | 72/156 [02:57<03:29,  2.50s/it] 47%|████▋     | 73/156 [02:59<03:26,  2.49s/it] 47%|████▋     | 74/156 [03:02<03:23,  2.48s/it] 48%|████▊     | 75/156 [03:04<03:19,  2.46s/it] 49%|████▊     | 76/156 [03:07<03:16,  2.45s/it] 49%|████▉     | 77/156 [03:09<03:13,  2.44s/it] 50%|█████     | 78/156 [03:12<03:10,  2.44s/it] 51%|█████     | 79/156 [03:14<03:08,  2.45s/it] 51%|█████▏    | 80/156 [03:16<03:04,  2.43s/it]                                                 51%|█████▏    | 80/156 [03:16<03:04,  2.43s/it] 52%|█████▏    | 81/156 [03:19<03:03,  2.44s/it] 53%|█████▎    | 82/156 [03:21<03:00,  2.45s/it] 53%|█████▎    | 83/156 [03:24<02:59,  2.45s/it] 54%|█████▍    | 84/156 [03:26<02:56,  2.46s/it] 54%|█████▍    | 85/156 [03:29<02:54,  2.46s/it] 55%|█████▌    | 86/156 [03:31<02:51,  2.45s/it] 56%|█████▌    | 87/156 [03:34<02:49,  2.45s/it] 56%|█████▋    | 88/156 [03:36<02:46,  2.45s/it] 57%|█████▋    | 89/156 [03:39<02:44,  2.46s/it] 58%|█████▊    | 90/156 [03:41<02:42,  2.46s/it]                                                 58%|█████▊    | 90/156 [03:41<02:42,  2.46s/it] 58%|█████▊    | 91/156 [03:43<02:39,  2.45s/it] 59%|█████▉    | 92/156 [03:46<02:36,  2.45s/it] 60%|█████▉    | 93/156 [03:48<02:34,  2.45s/it] 60%|██████    | 94/156 [03:51<02:33,  2.47s/it] 61%|██████    | 95/156 [03:53<02:30,  2.47s/it] 62%|██████▏   | 96/156 [03:56<02:28,  2.48s/it] 62%|██████▏   | 97/156 [03:58<02:25,  2.47s/it] 63%|██████▎   | 98/156 [04:01<02:22,  2.46s/it] 63%|██████▎   | 99/156 [04:03<02:20,  2.47s/it] 64%|██████▍   | 100/156 [04:06<02:18,  2.47s/it]                                                  64%|██████▍   | 100/156 [04:06<02:18,  2.47s/it] 65%|██████▍   | 101/156 [04:08<02:15,  2.47s/it] 65%|██████▌   | 102/156 [04:11<02:13,  2.48s/it] 66%|██████▌   | 103/156 [04:13<02:10,  2.47s/it] 67%|██████▋   | 104/156 [04:16<02:08,  2.47s/it] 67%|██████▋   | 105/156 [04:18<02:06,  2.47s/it] 68%|██████▊   | 106/156 [04:20<02:03,  2.46s/it] 69%|██████▊   | 107/156 [04:23<02:00,  2.47s/it] 69%|██████▉   | 108/156 [04:25<01:59,  2.48s/it] 70%|██████▉   | 109/156 [04:28<01:56,  2.48s/it] 71%|███████   | 110/156 [04:30<01:53,  2.48s/it]                                                  71%|███████   | 110/156 [04:30<01:53,  2.48s/it] 71%|███████   | 111/156 [04:33<01:50,  2.46s/it] 72%|███████▏  | 112/156 [04:35<01:48,  2.46s/it] 72%|███████▏  | 113/156 [04:38<01:46,  2.47s/it] 73%|███████▎  | 114/156 [04:40<01:43,  2.48s/it] 74%|███████▎  | 115/156 [04:43<01:41,  2.47s/it] 74%|███████▍  | 116/156 [04:45<01:39,  2.48s/it] 75%|███████▌  | 117/156 [04:48<01:36,  2.48s/it] 76%|███████▌  | 118/156 [04:50<01:34,  2.48s/it] 76%|███████▋  | 119/156 [04:53<01:31,  2.48s/it] 77%|███████▋  | 120/156 [04:55<01:29,  2.49s/it]                                                  77%|███████▋  | 120/156 [04:55<01:29,  2.49s/it] 78%|███████▊  | 121/156 [04:58<01:26,  2.48s/it] 78%|███████▊  | 122/156 [05:00<01:24,  2.49s/it] 79%|███████▉  | 123/156 [05:03<01:22,  2.49s/it] 79%|███████▉  | 124/156 [05:05<01:19,  2.49s/it] 80%|████████  | 125/156 [05:08<01:17,  2.49s/it] 81%|████████  | 126/156 [05:10<01:14,  2.49s/it] 81%|████████▏ | 127/156 [05:13<01:12,  2.49s/it] 82%|████████▏ | 128/156 [05:15<01:09,  2.48s/it] 83%|████████▎ | 129/156 [05:17<01:06,  2.47s/it] 83%|████████▎ | 130/156 [05:20<01:04,  2.48s/it]                                                  83%|████████▎ | 130/156 [05:20<01:04,  2.48s/it] 84%|████████▍ | 131/156 [05:22<01:01,  2.47s/it] 85%|████████▍ | 132/156 [05:25<00:59,  2.46s/it] 85%|████████▌ | 133/156 [05:27<00:56,  2.46s/it] 86%|████████▌ | 134/156 [05:30<00:54,  2.47s/it] 87%|████████▋ | 135/156 [05:32<00:51,  2.47s/it] 87%|████████▋ | 136/156 [05:35<00:49,  2.48s/it] 88%|████████▊ | 137/156 [05:37<00:46,  2.47s/it] 88%|████████▊ | 138/156 [05:40<00:44,  2.45s/it] 89%|████████▉ | 139/156 [05:42<00:41,  2.46s/it] 90%|████████▉ | 140/156 [05:45<00:39,  2.46s/it]                                                  90%|████████▉ | 140/156 [05:45<00:39,  2.46s/it] 90%|█████████ | 141/156 [05:47<00:36,  2.46s/it] 91%|█████████ | 142/156 [05:50<00:34,  2.47s/it] 92%|█████████▏| 143/156 [05:52<00:32,  2.47s/it] 92%|█████████▏| 144/156 [05:54<00:29,  2.47s/it] 93%|█████████▎| 145/156 [05:57<00:27,  2.48s/it] 94%|█████████▎| 146/156 [05:59<00:24,  2.49s/it] 94%|█████████▍| 147/156 [06:02<00:22,  2.49s/it] 95%|█████████▍| 148/156 [06:04<00:19,  2.47s/it] 96%|█████████▌| 149/156 [06:07<00:17,  2.47s/it] 96%|█████████▌| 150/156 [06:09<00:14,  2.46s/it]                                                  96%|█████████▌| 150/156 [06:09<00:14,  2.46s/it] 97%|█████████▋| 151/156 [06:12<00:12,  2.46s/it] 97%|█████████▋| 152/156 [06:14<00:09,  2.47s/it] 98%|█████████▊| 153/156 [06:17<00:07,  2.45s/it] 99%|█████████▊| 154/156 [06:19<00:04,  2.44s/it] 99%|█████████▉| 155/156 [06:22<00:02,  2.45s/it]100%|██████████| 156/156 [06:24<00:00,  2.45s/it]                                                 100%|██████████| 156/156 [06:24<00:00,  2.45s/it]100%|██████████| 156/156 [06:24<00:00,  2.46s/it]
  0%|          | 0/90 [00:00<?, ?it/s]  2%|▏         | 2/90 [00:01<00:53,  1.63it/s]  3%|▎         | 3/90 [00:02<01:19,  1.10it/s]  4%|▍         | 4/90 [00:03<01:28,  1.02s/it]  6%|▌         | 5/90 [00:04<01:32,  1.09s/it]  7%|▋         | 6/90 [00:06<01:35,  1.14s/it]  8%|▊         | 7/90 [00:07<01:34,  1.13s/it]  9%|▉         | 8/90 [00:08<01:34,  1.15s/it] 10%|█         | 9/90 [00:10<01:42,  1.26s/it] 11%|█         | 10/90 [00:11<01:47,  1.34s/it] 12%|█▏        | 11/90 [00:12<01:41,  1.28s/it] 13%|█▎        | 12/90 [00:13<01:37,  1.25s/it] 14%|█▍        | 13/90 [00:15<01:36,  1.25s/it] 16%|█▌        | 14/90 [00:16<01:35,  1.26s/it] 17%|█▋        | 15/90 [00:17<01:41,  1.35s/it] 18%|█▊        | 16/90 [00:19<01:34,  1.28s/it] 19%|█▉        | 17/90 [00:20<01:30,  1.24s/it] 20%|██        | 18/90 [00:21<01:28,  1.23s/it] 21%|██        | 19/90 [00:22<01:25,  1.21s/it] 22%|██▏       | 20/90 [00:23<01:22,  1.18s/it] 23%|██▎       | 21/90 [00:24<01:20,  1.17s/it] 24%|██▍       | 22/90 [00:26<01:19,  1.16s/it] 26%|██▌       | 23/90 [00:27<01:24,  1.27s/it] 27%|██▋       | 24/90 [00:28<01:25,  1.30s/it] 28%|██▊       | 25/90 [00:30<01:21,  1.26s/it] 29%|██▉       | 26/90 [00:31<01:18,  1.23s/it] 30%|███       | 27/90 [00:32<01:15,  1.20s/it] 31%|███       | 28/90 [00:33<01:12,  1.17s/it] 32%|███▏      | 29/90 [00:34<01:13,  1.20s/it] 33%|███▎      | 30/90 [00:36<01:28,  1.47s/it] 34%|███▍      | 31/90 [00:39<01:40,  1.70s/it] 36%|███▌      | 32/90 [00:41<01:47,  1.86s/it] 37%|███▋      | 33/90 [00:43<01:51,  1.96s/it] 38%|███▊      | 34/90 [00:45<01:54,  2.05s/it] 39%|███▉      | 35/90 [00:47<01:55,  2.10s/it] 40%|████      | 36/90 [00:50<01:53,  2.11s/it] 41%|████      | 37/90 [00:52<01:53,  2.15s/it] 42%|████▏     | 38/90 [00:54<01:52,  2.17s/it] 43%|████▎     | 39/90 [00:56<01:51,  2.19s/it] 44%|████▍     | 40/90 [00:58<01:49,  2.19s/it] 46%|████▌     | 41/90 [01:01<01:48,  2.21s/it] 47%|████▋     | 42/90 [01:03<01:46,  2.21s/it] 48%|████▊     | 43/90 [01:05<01:44,  2.22s/it] 49%|████▉     | 44/90 [01:07<01:40,  2.19s/it] 50%|█████     | 45/90 [01:10<01:39,  2.21s/it] 51%|█████     | 46/90 [01:12<01:37,  2.22s/it] 52%|█████▏    | 47/90 [01:14<01:35,  2.22s/it] 53%|█████▎    | 48/90 [01:16<01:33,  2.23s/it] 54%|█████▍    | 49/90 [01:19<01:31,  2.23s/it] 56%|█████▌    | 50/90 [01:21<01:29,  2.24s/it] 57%|█████▋    | 51/90 [01:23<01:27,  2.23s/it] 58%|█████▊    | 52/90 [01:25<01:23,  2.19s/it] 59%|█████▉    | 53/90 [01:27<01:20,  2.19s/it] 60%|██████    | 54/90 [01:29<01:19,  2.20s/it] 61%|██████    | 55/90 [01:32<01:17,  2.21s/it] 62%|██████▏   | 56/90 [01:34<01:15,  2.21s/it] 63%|██████▎   | 57/90 [01:36<01:12,  2.21s/it] 64%|██████▍   | 58/90 [01:38<01:10,  2.22s/it] 66%|██████▌   | 59/90 [01:40<01:07,  2.18s/it] 67%|██████▋   | 60/90 [01:43<01:05,  2.19s/it] 68%|██████▊   | 61/90 [01:44<00:54,  1.88s/it] 69%|██████▉   | 62/90 [01:45<00:44,  1.60s/it] 70%|███████   | 63/90 [01:46<00:38,  1.41s/it] 71%|███████   | 64/90 [01:47<00:32,  1.27s/it] 72%|███████▏  | 65/90 [01:48<00:29,  1.17s/it] 73%|███████▎  | 66/90 [01:49<00:26,  1.09s/it] 74%|███████▍  | 67/90 [01:49<00:23,  1.03s/it] 76%|███████▌  | 68/90 [01:50<00:22,  1.01s/it] 77%|███████▋  | 69/90 [01:51<00:20,  1.03it/s] 78%|███████▊  | 70/90 [01:52<00:19,  1.02it/s] 79%|███████▉  | 71/90 [01:53<00:18,  1.03it/s] 80%|████████  | 72/90 [01:55<00:23,  1.28s/it] 81%|████████  | 73/90 [01:56<00:19,  1.17s/it] 82%|████████▏ | 74/90 [01:57<00:18,  1.17s/it] 83%|████████▎ | 75/90 [01:58<00:16,  1.08s/it] 84%|████████▍ | 76/90 [01:59<00:14,  1.03s/it] 86%|████████▌ | 77/90 [02:00<00:12,  1.02it/s] 87%|████████▋ | 78/90 [02:01<00:11,  1.04it/s] 88%|████████▊ | 79/90 [02:02<00:10,  1.06it/s] 89%|████████▉ | 80/90 [02:03<00:09,  1.07it/s] 90%|█████████ | 81/90 [02:05<00:11,  1.26s/it] 91%|█████████ | 82/90 [02:06<00:09,  1.17s/it] 92%|█████████▏| 83/90 [02:07<00:07,  1.10s/it] 93%|█████████▎| 84/90 [02:08<00:06,  1.13s/it] 94%|█████████▍| 85/90 [02:09<00:05,  1.11s/it] 96%|█████████▌| 86/90 [02:10<00:04,  1.08s/it] 97%|█████████▋| 87/90 [02:11<00:03,  1.05s/it] 98%|█████████▊| 88/90 [02:12<00:02,  1.05s/it] 99%|█████████▉| 89/90 [02:13<00:01,  1.03s/it]100%|██████████| 90/90 [02:14<00:00,  1.09s/it]100%|██████████| 90/90 [02:27<00:00,  1.63s/it]
[rank0]:[W226 00:10:42.040979531 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
+ sleep 5
+ deepspeed --num_gpus=2 --master_port 28536 src/run_uie_lora.py --do_train --do_predict --predict_with_generate --model_name_or_path logs_and_outputs/order_1/outputs/3-yahoo/adapter --data_dir CL_Benchmark --task_config_dir configs/order1_configs/agnews --instruction_file configs/instruction_config.json --instruction_strategy single --output_dir logs_and_outputs/order_1/outputs/4-agnews --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 4 --learning_rate 1e-03 --num_train_epochs 1 --deepspeed configs/ds_configs/stage2.config --run_name order1_round4 --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --evaluation_strategy no --save_strategy no --save_steps 1500 --lamda_1 0.5 --lamda_2 0
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/accelerate/utils/torch_xla.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/accelerate/utils/torch_xla.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/accelerate/utils/torch_xla.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/accelerate/utils/torch_xla.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 858.55it/s]
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 840.77it/s]
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  return func(*args, **kwargs)
[rank0]:[W226 00:11:17.038977503 ProcessGroupNCCL.cpp:5138] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
  0%|          | 0/62 [00:00<?, ?it/s]/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/autograd/graph.py:865: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:76.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/torch/autograd/graph.py:865: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:76.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1830: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
/workspace/envs/MLLMs/yangye/conda_envs/lora/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1830: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
  2%|▏         | 1/62 [00:02<02:49,  2.78s/it]  3%|▎         | 2/62 [00:05<02:32,  2.55s/it]  5%|▍         | 3/62 [00:07<02:26,  2.49s/it]  6%|▋         | 4/62 [00:09<02:21,  2.44s/it]  8%|▊         | 5/62 [00:12<02:17,  2.41s/it] 10%|▉         | 6/62 [00:14<02:14,  2.40s/it] 11%|█▏        | 7/62 [00:17<02:11,  2.39s/it] 13%|█▎        | 8/62 [00:19<02:08,  2.37s/it] 15%|█▍        | 9/62 [00:21<02:05,  2.38s/it] 16%|█▌        | 10/62 [00:24<02:03,  2.37s/it]                                                16%|█▌        | 10/62 [00:24<02:03,  2.37s/it] 18%|█▊        | 11/62 [00:26<02:01,  2.38s/it] 19%|█▉        | 12/62 [00:28<01:58,  2.37s/it] 21%|██        | 13/62 [00:31<01:56,  2.37s/it] 23%|██▎       | 14/62 [00:33<01:53,  2.37s/it] 24%|██▍       | 15/62 [00:36<01:51,  2.37s/it] 26%|██▌       | 16/62 [00:38<01:49,  2.37s/it] 27%|██▋       | 17/62 [00:40<01:46,  2.37s/it] 29%|██▉       | 18/62 [00:43<01:44,  2.37s/it] 31%|███       | 19/62 [00:45<01:41,  2.36s/it] 32%|███▏      | 20/62 [00:47<01:39,  2.36s/it]                                                32%|███▏      | 20/62 [00:47<01:39,  2.36s/it] 34%|███▍      | 21/62 [00:50<01:36,  2.36s/it] 35%|███▌      | 22/62 [00:52<01:34,  2.36s/it] 37%|███▋      | 23/62 [00:54<01:32,  2.36s/it] 39%|███▊      | 24/62 [00:57<01:29,  2.36s/it] 40%|████      | 25/62 [00:59<01:27,  2.36s/it] 42%|████▏     | 26/62 [01:01<01:25,  2.36s/it] 44%|████▎     | 27/62 [01:04<01:22,  2.36s/it] 45%|████▌     | 28/62 [01:06<01:20,  2.36s/it] 47%|████▋     | 29/62 [01:09<01:18,  2.37s/it] 48%|████▊     | 30/62 [01:11<01:15,  2.37s/it]                                                48%|████▊     | 30/62 [01:11<01:15,  2.37s/it] 50%|█████     | 31/62 [01:13<01:13,  2.37s/it] 52%|█████▏    | 32/62 [01:16<01:10,  2.37s/it] 53%|█████▎    | 33/62 [01:18<01:08,  2.37s/it] 55%|█████▍    | 34/62 [01:20<01:06,  2.37s/it] 56%|█████▋    | 35/62 [01:23<01:03,  2.37s/it] 58%|█████▊    | 36/62 [01:25<01:01,  2.36s/it] 60%|█████▉    | 37/62 [01:28<00:59,  2.36s/it] 61%|██████▏   | 38/62 [01:30<00:56,  2.36s/it] 63%|██████▎   | 39/62 [01:32<00:54,  2.36s/it] 65%|██████▍   | 40/62 [01:35<00:51,  2.36s/it]                                                65%|██████▍   | 40/62 [01:35<00:51,  2.36s/it] 66%|██████▌   | 41/62 [01:37<00:49,  2.36s/it] 68%|██████▊   | 42/62 [01:39<00:47,  2.37s/it] 69%|██████▉   | 43/62 [01:42<00:44,  2.36s/it] 71%|███████   | 44/62 [01:44<00:42,  2.37s/it] 73%|███████▎  | 45/62 [01:46<00:40,  2.36s/it] 74%|███████▍  | 46/62 [01:49<00:37,  2.36s/it] 76%|███████▌  | 47/62 [01:51<00:35,  2.36s/it] 77%|███████▋  | 48/62 [01:54<00:33,  2.36s/it] 79%|███████▉  | 49/62 [01:56<00:30,  2.37s/it] 81%|████████  | 50/62 [01:58<00:28,  2.36s/it]                                                81%|████████  | 50/62 [01:58<00:28,  2.36s/it] 82%|████████▏ | 51/62 [02:01<00:25,  2.36s/it] 84%|████████▍ | 52/62 [02:03<00:23,  2.36s/it] 85%|████████▌ | 53/62 [02:05<00:21,  2.36s/it] 87%|████████▋ | 54/62 [02:08<00:18,  2.36s/it] 89%|████████▊ | 55/62 [02:10<00:16,  2.36s/it] 90%|█████████ | 56/62 [02:12<00:14,  2.37s/it] 92%|█████████▏| 57/62 [02:15<00:11,  2.37s/it] 94%|█████████▎| 58/62 [02:17<00:09,  2.37s/it] 95%|█████████▌| 59/62 [02:20<00:07,  2.37s/it] 97%|█████████▋| 60/62 [02:22<00:04,  2.37s/it]                                                97%|█████████▋| 60/62 [02:22<00:04,  2.37s/it] 98%|█████████▊| 61/62 [02:24<00:02,  2.37s/it]100%|██████████| 62/62 [02:27<00:00,  2.37s/it]                                               100%|██████████| 62/62 [02:27<00:00,  2.37s/it]100%|██████████| 62/62 [02:27<00:00,  2.37s/it]
  0%|          | 0/119 [00:00<?, ?it/s]  2%|▏         | 2/119 [00:01<01:09,  1.68it/s]  3%|▎         | 3/119 [00:02<01:45,  1.10it/s]  3%|▎         | 4/119 [00:03<01:58,  1.03s/it]  4%|▍         | 5/119 [00:05<02:05,  1.10s/it]  5%|▌         | 6/119 [00:06<02:08,  1.13s/it]  6%|▌         | 7/119 [00:07<02:06,  1.13s/it]  7%|▋         | 8/119 [00:08<02:07,  1.15s/it]  8%|▊         | 9/119 [00:10<02:19,  1.27s/it]  8%|▊         | 10/119 [00:11<02:26,  1.34s/it]  9%|▉         | 11/119 [00:12<02:18,  1.28s/it] 10%|█         | 12/119 [00:13<02:13,  1.25s/it] 11%|█         | 13/119 [00:15<02:12,  1.25s/it] 12%|█▏        | 14/119 [00:16<02:12,  1.26s/it] 13%|█▎        | 15/119 [00:17<02:20,  1.35s/it] 13%|█▎        | 16/119 [00:19<02:11,  1.27s/it] 14%|█▍        | 17/119 [00:20<02:07,  1.25s/it] 15%|█▌        | 18/119 [00:21<02:04,  1.24s/it] 16%|█▌        | 19/119 [00:22<02:01,  1.21s/it] 17%|█▋        | 20/119 [00:23<01:58,  1.19s/it] 18%|█▊        | 21/119 [00:24<01:54,  1.17s/it] 18%|█▊        | 22/119 [00:26<01:52,  1.16s/it] 19%|█▉        | 23/119 [00:27<02:00,  1.26s/it] 20%|██        | 24/119 [00:28<02:02,  1.29s/it] 21%|██        | 25/119 [00:30<01:58,  1.26s/it] 22%|██▏       | 26/119 [00:31<01:54,  1.23s/it] 23%|██▎       | 27/119 [00:32<01:50,  1.20s/it] 24%|██▎       | 28/119 [00:33<01:46,  1.17s/it] 24%|██▍       | 29/119 [00:34<01:47,  1.20s/it] 25%|██▌       | 30/119 [00:36<02:10,  1.47s/it] 26%|██▌       | 31/119 [00:39<02:30,  1.71s/it] 27%|██▋       | 32/119 [00:41<02:42,  1.87s/it] 28%|██▊       | 33/119 [00:43<02:49,  1.98s/it] 29%|██▊       | 34/119 [00:45<02:54,  2.06s/it] 29%|██▉       | 35/119 [00:48<02:57,  2.11s/it] 30%|███       | 36/119 [00:50<02:55,  2.12s/it] 31%|███       | 37/119 [00:52<02:56,  2.15s/it] 32%|███▏      | 38/119 [00:54<02:56,  2.18s/it] 33%|███▎      | 39/119 [00:56<02:55,  2.19s/it] 34%|███▎      | 40/119 [00:59<02:53,  2.19s/it] 34%|███▍      | 41/119 [01:01<02:52,  2.21s/it] 35%|███▌      | 42/119 [01:03<02:50,  2.22s/it] 36%|███▌      | 43/119 [01:05<02:49,  2.23s/it] 37%|███▋      | 44/119 [01:07<02:45,  2.20s/it] 38%|███▊      | 45/119 [01:10<02:43,  2.21s/it] 39%|███▊      | 46/119 [01:12<02:42,  2.22s/it] 39%|███▉      | 47/119 [01:14<02:40,  2.23s/it] 40%|████      | 48/119 [01:16<02:38,  2.23s/it] 41%|████      | 49/119 [01:19<02:36,  2.24s/it] 42%|████▏     | 50/119 [01:21<02:35,  2.25s/it] 43%|████▎     | 51/119 [01:23<02:32,  2.24s/it] 44%|████▎     | 52/119 [01:25<02:28,  2.21s/it] 45%|████▍     | 53/119 [01:27<02:25,  2.21s/it] 45%|████▌     | 54/119 [01:30<02:23,  2.21s/it] 46%|████▌     | 55/119 [01:32<02:21,  2.21s/it] 47%|████▋     | 56/119 [01:34<02:19,  2.22s/it] 48%|████▊     | 57/119 [01:36<02:17,  2.21s/it] 49%|████▊     | 58/119 [01:39<02:15,  2.22s/it] 50%|████▉     | 59/119 [01:41<02:11,  2.20s/it] 50%|█████     | 60/119 [01:43<02:09,  2.20s/it] 51%|█████▏    | 61/119 [01:44<01:49,  1.89s/it] 52%|█████▏    | 62/119 [01:45<01:31,  1.61s/it] 53%|█████▎    | 63/119 [01:46<01:19,  1.42s/it] 54%|█████▍    | 64/119 [01:47<01:09,  1.27s/it] 55%|█████▍    | 65/119 [01:48<01:02,  1.16s/it] 55%|█████▌    | 66/119 [01:49<00:57,  1.08s/it] 56%|█████▋    | 67/119 [01:50<00:52,  1.02s/it] 57%|█████▋    | 68/119 [01:51<00:51,  1.00s/it] 58%|█████▊    | 69/119 [01:51<00:48,  1.03it/s] 59%|█████▉    | 70/119 [01:52<00:47,  1.03it/s] 60%|█████▉    | 71/119 [01:53<00:46,  1.04it/s] 61%|██████    | 72/119 [01:55<01:00,  1.28s/it] 61%|██████▏   | 73/119 [01:56<00:53,  1.17s/it] 62%|██████▏   | 74/119 [01:58<00:52,  1.17s/it] 63%|██████▎   | 75/119 [01:58<00:47,  1.08s/it] 64%|██████▍   | 76/119 [01:59<00:43,  1.02s/it] 65%|██████▍   | 77/119 [02:00<00:41,  1.02it/s] 66%|██████▌   | 78/119 [02:01<00:39,  1.05it/s] 66%|██████▋   | 79/119 [02:02<00:37,  1.06it/s] 67%|██████▋   | 80/119 [02:03<00:36,  1.07it/s] 68%|██████▊   | 81/119 [02:05<00:47,  1.26s/it] 69%|██████▉   | 82/119 [02:06<00:43,  1.16s/it] 70%|██████▉   | 83/119 [02:07<00:39,  1.10s/it] 71%|███████   | 84/119 [02:08<00:39,  1.12s/it] 71%|███████▏  | 85/119 [02:09<00:37,  1.11s/it] 72%|███████▏  | 86/119 [02:10<00:35,  1.07s/it] 73%|███████▎  | 87/119 [02:11<00:33,  1.04s/it] 74%|███████▍  | 88/119 [02:12<00:32,  1.05s/it] 75%|███████▍  | 89/119 [02:13<00:30,  1.03s/it] 76%|███████▌  | 90/119 [02:14<00:31,  1.09s/it] 76%|███████▋  | 91/119 [02:15<00:28,  1.00s/it] 77%|███████▋  | 92/119 [02:16<00:24,  1.09it/s] 78%|███████▊  | 93/119 [02:17<00:22,  1.16it/s] 79%|███████▉  | 94/119 [02:17<00:21,  1.15it/s] 80%|███████▉  | 95/119 [02:18<00:20,  1.17it/s] 81%|████████  | 96/119 [02:19<00:19,  1.16it/s] 82%|████████▏ | 97/119 [02:20<00:19,  1.15it/s] 82%|████████▏ | 98/119 [02:21<00:18,  1.12it/s] 83%|████████▎ | 99/119 [02:22<00:16,  1.19it/s] 84%|████████▍ | 100/119 [02:23<00:16,  1.17it/s] 85%|████████▍ | 101/119 [02:24<00:17,  1.02it/s] 86%|████████▌ | 102/119 [02:25<00:17,  1.01s/it] 87%|████████▋ | 103/119 [02:26<00:15,  1.00it/s] 87%|████████▋ | 104/119 [02:27<00:15,  1.01s/it] 88%|████████▊ | 105/119 [02:28<00:13,  1.08it/s] 89%|████████▉ | 106/119 [02:29<00:12,  1.05it/s] 90%|████████▉ | 107/119 [02:29<00:10,  1.15it/s] 91%|█████████ | 108/119 [02:30<00:09,  1.21it/s] 92%|█████████▏| 109/119 [02:31<00:08,  1.18it/s] 92%|█████████▏| 110/119 [02:32<00:07,  1.18it/s] 93%|█████████▎| 111/119 [02:32<00:06,  1.25it/s] 94%|█████████▍| 112/119 [02:33<00:05,  1.30it/s] 95%|█████████▍| 113/119 [02:34<00:04,  1.34it/s] 96%|█████████▌| 114/119 [02:35<00:03,  1.35it/s] 97%|█████████▋| 115/119 [02:36<00:03,  1.24it/s] 97%|█████████▋| 116/119 [02:36<00:02,  1.28it/s] 98%|█████████▊| 117/119 [02:37<00:01,  1.16it/s] 99%|█████████▉| 118/119 [02:38<00:00,  1.23it/s]100%|██████████| 119/119 [02:39<00:00,  1.03it/s]100%|██████████| 119/119 [02:56<00:00,  1.48s/it]
[rank0]:[W226 00:16:44.642147504 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
+ echo Finished.
